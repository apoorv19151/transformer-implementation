{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NC3cHFqScD2y"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Optional, Tuple, Dict\n",
        "import unittest\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_angles(pos: np.ndarray, k: np.ndarray, d: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the raw angle terms for positional encoding.\n",
        "\n",
        "    Formula (from \"Attention is All You Need\"):\n",
        "        angle(pos, k) = pos / (10000^(2i/d))\n",
        "\n",
        "    where:\n",
        "        pos (np.ndarray): Column vector of positions, shape (N, 1)\n",
        "                          e.g., [[0], [1], ..., [N-1]]\n",
        "        k   (np.ndarray): Row vector of dimension indices, shape (1, d)\n",
        "                          e.g., [[0, 1, ..., d-1]]\n",
        "        d   (int): Dimension of the embedding/positional encoding.\n",
        "        i   (np.ndarray): Frequency index, computed as k // 2.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Angle matrix of shape (N, d), where each row\n",
        "                    corresponds to a position and each column to a dimension.\n",
        "    \"\"\"\n",
        "    i = k // 2\n",
        "    angles = pos / np.power(10000, (2 * i) / float(d))\n",
        "    return angles\n"
      ],
      "metadata": {
        "id": "LBXHBbG1t7Kp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestGetAngles(unittest.TestCase):\n",
        "\n",
        "    def test_pos_zero(self):\n",
        "        pos = np.array([[0]])\n",
        "        k = np.arange(8)[np.newaxis, :]\n",
        "        angles = get_angles(pos, k, 8)\n",
        "        self.assertTrue(np.allclose(angles, np.zeros((1, 8))))\n",
        "\n",
        "    def test_small_values(self):\n",
        "        pos = np.array([[2]])\n",
        "        k = np.arange(4)[np.newaxis, :]\n",
        "        angles = get_angles(pos, k, 4)\n",
        "        expected = np.array([[2.0, 2.0, 0.02, 0.02]])\n",
        "        self.assertTrue(np.allclose(angles, expected, atol=1e-4))\n",
        "\n",
        "    def test_symmetry_even_odd(self):\n",
        "        pos = np.array([[5]])\n",
        "        k = np.arange(6)[np.newaxis, :]\n",
        "        angles = get_angles(pos, k, 6)\n",
        "        self.assertTrue(np.allclose(angles[0,4], angles[0,5]))\n",
        "\n",
        "    def test_increasing_d(self):\n",
        "        pos = np.array([[10]])\n",
        "        k = np.array([[2]])\n",
        "        small_d = get_angles(pos, k, 8)\n",
        "        large_d = get_angles(pos, k, 512)\n",
        "        self.assertGreater(large_d, small_d)\n",
        "\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestGetAngles))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLsJzsK2a7T8",
        "outputId": "c34addde-c9eb-4509-a02d-d45802f229f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "....\n",
            "----------------------------------------------------------------------\n",
            "Ran 4 tests in 0.004s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=4 errors=0 failures=0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encoding(positions: int, d: int) -> tf.Tensor:\n",
        "    \"\"\"\n",
        "    Precomputes a matrix with all the positional encodings.\n",
        "\n",
        "    Arguments:\n",
        "        positions (int) -- Maximum number of positions to be encoded\n",
        "        d (int) -- Encoding size\n",
        "\n",
        "    Returns:\n",
        "        pos_encoding (tf.Tensor) -- Tensor of shape (1, positions, d) containing the positional encodings\n",
        "    \"\"\"\n",
        "    # Create position indices (positions, 1)\n",
        "    pos: np.ndarray = np.arange(positions)[:, np.newaxis]\n",
        "\n",
        "    # Create dimension indices (1, d)\n",
        "    k: np.ndarray = np.arange(d)[np.newaxis, :]\n",
        "\n",
        "    # Compute raw angles using get_angles\n",
        "    angle_rads: np.ndarray = get_angles(pos, k, d)\n",
        "\n",
        "    # Apply sin to even indices (2i)\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # Apply cos to odd indices (2i+1)\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    # Add batch dimension\n",
        "    pos_encoding: np.ndarray = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
      ],
      "metadata": {
        "id": "WD1bNzzZgXiw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestPositionalEncoding(unittest.TestCase):\n",
        "\n",
        "    def test_shape(self):\n",
        "        positions = 10\n",
        "        d = 8\n",
        "        pos_enc = positional_encoding(positions, d)\n",
        "        # Check output shape (1, positions, d)\n",
        "        self.assertEqual(pos_enc.shape, (1, positions, d))\n",
        "\n",
        "    def test_sin_cos_ranges(self):\n",
        "        positions = 5\n",
        "        d = 6\n",
        "        pos_enc = positional_encoding(positions, d).numpy()  # convert to NumPy for easy checks\n",
        "\n",
        "        # Even indices: sin\n",
        "        even_vals = pos_enc[0, :, 0::2]\n",
        "        # Should be between -1 and 1\n",
        "        self.assertTrue(np.all(even_vals >= -1.0) and np.all(even_vals <= 1.0))\n",
        "\n",
        "        # Odd indices: cos\n",
        "        odd_vals = pos_enc[0, :, 1::2]\n",
        "        self.assertTrue(np.all(odd_vals >= -1.0) and np.all(odd_vals <= 1.0))\n",
        "\n",
        "    def test_batch_dimension(self):\n",
        "        positions = 7\n",
        "        d = 4\n",
        "        pos_enc = positional_encoding(positions, d)\n",
        "        # Batch size should be 1\n",
        "        self.assertEqual(pos_enc.shape[0], 1)\n",
        "\n",
        "# Run the tests in Colab\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestPositionalEncoding))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYm-Hiu-gwHq",
        "outputId": "21b09669-c556-4ba5-dd2e-00fcfdfebe5b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "...\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.033s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(decoder_token_ids: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"\n",
        "    Creates a padding mask for Transformer attention.\n",
        "\n",
        "    This mask will have 1s for real tokens and 0s for padding tokens (usually 0 in the input),\n",
        "    and adds an extra dimension to allow broadcasting in the attention mechanism.\n",
        "\n",
        "    Arguments:\n",
        "        decoder_token_ids (tf.Tensor): Tensor of shape (batch_size, seq_len) containing token IDs,\n",
        "                                       where padding tokens are 0.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Binary mask of shape (batch_size, 1, seq_len), where\n",
        "                   1 indicates a real token (attend to it) and\n",
        "                   0 indicates padding (ignore in attention).\n",
        "    \"\"\"\n",
        "    # 1 for real tokens, 0 for padding tokens\n",
        "    seq_mask = 1 - tf.cast(tf.math.equal(decoder_token_ids, 0), tf.float32)\n",
        "\n",
        "    # Add extra dimension for broadcasting in attention\n",
        "    return seq_mask[:, tf.newaxis, :]\n",
        "\n",
        "\n",
        "def create_look_ahead_mask(sequence_length: int) -> tf.Tensor:\n",
        "    \"\"\"\n",
        "    Creates a look-ahead mask for decoder self-attention.\n",
        "\n",
        "    The mask prevents a position from attending to future positions.\n",
        "    Returns a lower-triangular matrix of ones with shape (1, sequence_length, sequence_length).\n",
        "\n",
        "    Arguments:\n",
        "        sequence_length (int): Length of the sequence to mask.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Tensor of shape (1, sequence_length, sequence_length) where\n",
        "                   1 indicates allowed attention and 0 indicates masked future positions.\n",
        "    \"\"\"\n",
        "    mask = tf.linalg.band_part(tf.ones((1, sequence_length, sequence_length)), -1, 0)\n",
        "    return mask"
      ],
      "metadata": {
        "id": "Vm-56JBIhf3s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q: tf.Tensor, k: tf.Tensor, v: tf.Tensor, mask: tf.Tensor = None):\n",
        "    \"\"\"\n",
        "    Calculate the scaled dot-product attention.\n",
        "\n",
        "    Arguments:\n",
        "        q -- query tensor of shape (..., seq_len_q, depth)\n",
        "        k -- key tensor of shape (..., seq_len_k, depth)\n",
        "        v -- value tensor of shape (..., seq_len_v, depth_v)\n",
        "        mask -- Float tensor broadcastable to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        output -- attention output tensor of shape (..., seq_len_q, depth_v)\n",
        "        attention_weights -- attention weights tensor of shape (..., seq_len_q, seq_len_k)\n",
        "    \"\"\"\n",
        "    # Dot product of queries and keys\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    # Scale by sqrt(depth of key)\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # Add mask (if any)\n",
        "    if mask is not None:\n",
        "        # mask contains 1s for valid positions, 0s for masked positions\n",
        "        # multiply by -1e9 so that softmax of masked positions â‰ˆ 0\n",
        "        scaled_attention_logits += (mask - 1) * 1e9\n",
        "\n",
        "    # Softmax along last axis (seq_len_k)\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "    # Multiply by values\n",
        "    output = tf.matmul(attention_weights, v)\n",
        "\n",
        "    return output, attention_weights\n"
      ],
      "metadata": {
        "id": "fbileT2inLQG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestScaledDotProductAttention(unittest.TestCase):\n",
        "\n",
        "    def test_output_shapes(self):\n",
        "        # q, k, v shapes: (batch_size, seq_len, depth)\n",
        "        q = tf.constant(np.random.rand(2, 3, 4), dtype=tf.float32)\n",
        "        k = tf.constant(np.random.rand(2, 3, 4), dtype=tf.float32)\n",
        "        v = tf.constant(np.random.rand(2, 3, 5), dtype=tf.float32)  # depth_v = 5\n",
        "\n",
        "        output, attn_weights = scaled_dot_product_attention(q, k, v)\n",
        "\n",
        "        # Check output shapes\n",
        "        self.assertEqual(output.shape, (2, 3, 5))\n",
        "        self.assertEqual(attn_weights.shape, (2, 3, 3))\n",
        "\n",
        "    def test_attention_sum_to_one(self):\n",
        "        # Attention weights along last axis should sum to 1\n",
        "        q = tf.constant([[ [1., 0.], [0., 1.] ]], dtype=tf.float32)  # shape (1, 2, 2)\n",
        "        k = tf.constant([[ [1., 0.], [0., 1.] ]], dtype=tf.float32)\n",
        "        v = tf.constant([[ [1., 2.], [3., 4.] ]], dtype=tf.float32)\n",
        "\n",
        "        _, attn_weights = scaled_dot_product_attention(q, k, v)\n",
        "\n",
        "        # Sum along last axis should be 1\n",
        "        attn_sum = tf.reduce_sum(attn_weights, axis=-1)\n",
        "        self.assertTrue(np.allclose(attn_sum.numpy(), np.ones(attn_sum.shape), atol=1e-6))\n",
        "\n",
        "    def test_masking_effect(self):\n",
        "        # Mask should zero out future tokens\n",
        "        q = tf.constant([[ [1., 0.], [0., 1.], [1., 1.] ]], dtype=tf.float32)\n",
        "        k = q\n",
        "        v = tf.constant([[ [1., 0.], [0., 1.], [1., 1.] ]], dtype=tf.float32)\n",
        "\n",
        "        # Create a look-ahead mask\n",
        "        mask = tf.constant([[ [1., 0., 0.],\n",
        "                              [1., 1., 0.],\n",
        "                              [1., 1., 1.] ]], dtype=tf.float32)\n",
        "\n",
        "        output, attn_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        # Masked positions in attention weights should be ~0\n",
        "        self.assertTrue(attn_weights[0,0,1] < 1e-6)\n",
        "        self.assertTrue(attn_weights[0,0,2] < 1e-6)\n",
        "        self.assertTrue(attn_weights[0,1,2] < 1e-6)\n",
        "\n",
        "# Run the tests\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestScaledDotProductAttention))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI_k9l-Rnol_",
        "outputId": "077468d4-b67d-4914-9b6a-2b9ba7a59ff8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "...\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.104s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FullyConnected(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Simple position-wise feed-forward network for Transformer layers.\n",
        "\n",
        "    Architecture:\n",
        "        Dense(fully_connected_dim, ReLU) -> Dense(embedding_dim)\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim: int, fully_connected_dim: int):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            embedding_dim (int): Dimension of the Transformer embedding (input & output).\n",
        "            fully_connected_dim (int): Hidden layer dimension for the feed-forward network.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dense1 = Dense(fully_connected_dim, activation='relu')\n",
        "        self.dense2 = Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass through the feed-forward network.\n",
        "\n",
        "        Arguments:\n",
        "            x (tf.Tensor): Input tensor of shape (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Output tensor of shape (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        x = self.dense1(x)\n",
        "        x = self.dense2(x)\n",
        "        return x\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Single Transformer Encoder layer consisting of:\n",
        "    - Multi-head self-attention\n",
        "    - Feed-forward network\n",
        "    - Residual connections + Layer Normalization\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 embedding_dim: int,\n",
        "                 num_heads: int,\n",
        "                 fully_connected_dim: int,\n",
        "                 dropout_rate: float = 0.1,\n",
        "                 layernorm_eps: float = 1e-6):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            embedding_dim (int): Dimension of the input embeddings.\n",
        "            num_heads (int): Number of attention heads in multi-head attention.\n",
        "            fully_connected_dim (int): Hidden layer size for the feed-forward network.\n",
        "            dropout_rate (float, optional): Dropout rate for feed-forward output. Defaults to 0.1.\n",
        "            layernorm_eps (float, optional): Epsilon for LayerNormalization. Defaults to 1e-6.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(num_heads=num_heads,\n",
        "                                      key_dim=embedding_dim,\n",
        "                                      dropout=dropout_rate)\n",
        "\n",
        "        self.ffn = FullyConnected(embedding_dim=embedding_dim,\n",
        "                                  fully_connected_dim=fully_connected_dim)\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
        "\n",
        "        self.dropout_ffn = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x: tf.Tensor, training: bool, mask: Optional[tf.Tensor] = None) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass for the Encoder Layer.\n",
        "\n",
        "        Arguments:\n",
        "            x (tf.Tensor): Input tensor of shape (batch_size, seq_len, embedding_dim)\n",
        "            training (bool): Whether the model is in training mode (applies dropout if True)\n",
        "            mask (tf.Tensor, optional): Attention mask of shape broadcastable to\n",
        "                                        (batch_size, num_heads, seq_len, seq_len). Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Output tensor of shape (batch_size, seq_len, embedding_dim)\n",
        "        \"\"\"\n",
        "        # Self-attention\n",
        "        self_mha_output = self.mha(query=x, value=x, key=x, attention_mask=mask)\n",
        "\n",
        "        # Skip connection + LayerNorm\n",
        "        skip_x_attention = self.layernorm1(x + self_mha_output)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ffn_output = self.ffn(skip_x_attention)\n",
        "\n",
        "        # Dropout\n",
        "        ffn_output = self.dropout_ffn(ffn_output, training=training)\n",
        "\n",
        "        # Skip connection + LayerNorm\n",
        "        encoder_layer_out = self.layernorm2(skip_x_attention + ffn_output)\n",
        "\n",
        "        return encoder_layer_out\n"
      ],
      "metadata": {
        "id": "wRf8QbDPpKIw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestEncoderLayer(unittest.TestCase):\n",
        "    def test_encoder_layer_output_shape(self):\n",
        "        batch_size = 2\n",
        "        seq_len = 4\n",
        "        embedding_dim = 8\n",
        "        num_heads = 2\n",
        "        ffn_dim = 16\n",
        "\n",
        "        encoder_layer = EncoderLayer(embedding_dim, num_heads, ffn_dim)\n",
        "\n",
        "        x = tf.constant(np.random.rand(batch_size, seq_len, embedding_dim), dtype=tf.float32)\n",
        "        mask = tf.constant(np.ones((batch_size, 1, 1, seq_len)), dtype=tf.float32)\n",
        "\n",
        "        output = encoder_layer(x, training=False, mask=mask)\n",
        "\n",
        "        # Check output shape\n",
        "        self.assertEqual(output.shape, (batch_size, seq_len, embedding_dim))\n",
        "\n",
        "    def test_encoder_layer_no_mask(self):\n",
        "        batch_size = 1\n",
        "        seq_len = 3\n",
        "        embedding_dim = 4\n",
        "        num_heads = 1\n",
        "        ffn_dim = 8\n",
        "\n",
        "        encoder_layer = EncoderLayer(embedding_dim, num_heads, ffn_dim)\n",
        "        x = tf.constant(np.random.rand(batch_size, seq_len, embedding_dim), dtype=tf.float32)\n",
        "\n",
        "        output = encoder_layer(x, training=False)\n",
        "        self.assertEqual(output.shape, (batch_size, seq_len, embedding_dim))\n",
        "\n",
        "# Run tests\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestEncoderLayer))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OQpwsKbpZ2f",
        "outputId": "cc311065-be7f-4835-fc6b-4886b075039b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "..\n",
            "----------------------------------------------------------------------\n",
            "Ran 2 tests in 0.379s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=2 errors=0 failures=0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Full Transformer Encoder consisting of:\n",
        "    1. Token embedding\n",
        "    2. Positional encoding\n",
        "    3. A stack of EncoderLayers\n",
        "    4. Dropout for regularization\n",
        "\n",
        "    The encoder transforms token indices into embeddings, adds positional information,\n",
        "    and passes the result through multiple encoder layers for contextual representation.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_layers: int,\n",
        "                 embedding_dim: int,\n",
        "                 num_heads: int,\n",
        "                 fully_connected_dim: int,\n",
        "                 input_vocab_size: int,\n",
        "                 maximum_position_encoding: int,\n",
        "                 dropout_rate: float = 0.1,\n",
        "                 layernorm_eps: float = 1e-6):\n",
        "        \"\"\"\n",
        "        Initializes the Transformer Encoder.\n",
        "\n",
        "        Arguments:\n",
        "            num_layers (int): Number of stacked EncoderLayer blocks.\n",
        "            embedding_dim (int): Dimensionality of the token embeddings.\n",
        "            num_heads (int): Number of attention heads for multi-head attention.\n",
        "            fully_connected_dim (int): Hidden size of the feed-forward network in each EncoderLayer.\n",
        "            input_vocab_size (int): Size of the input vocabulary (number of unique tokens).\n",
        "            maximum_position_encoding (int): Maximum number of positions to encode.\n",
        "            dropout_rate (float, optional): Dropout rate applied to embeddings and FFN. Defaults to 0.1.\n",
        "            layernorm_eps (float, optional): Epsilon value for LayerNormalization. Defaults to 1e-6.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Token embedding layer\n",
        "        self.embedding = Embedding(input_vocab_size, self.embedding_dim)\n",
        "\n",
        "        # Positional encoding matrix\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
        "\n",
        "        # Stack of EncoderLayer blocks\n",
        "        self.enc_layers = [\n",
        "            EncoderLayer(\n",
        "                embedding_dim=self.embedding_dim,\n",
        "                num_heads=num_heads,\n",
        "                fully_connected_dim=fully_connected_dim,\n",
        "                dropout_rate=dropout_rate,\n",
        "                layernorm_eps=layernorm_eps\n",
        "            )\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "        # Dropout layer applied after embedding\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x: tf.Tensor, training: bool, mask: Optional[tf.Tensor] = None) -> tf.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass through the Transformer Encoder.\n",
        "\n",
        "        Arguments:\n",
        "            x (tf.Tensor): Input tensor of shape (batch_size, seq_len), containing token indices.\n",
        "            training (bool): Boolean flag indicating training mode (for dropout).\n",
        "            mask (tf.Tensor, optional): Attention mask of shape broadcastable to\n",
        "                                        (batch_size, num_heads, seq_len, seq_len). Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor: Output tensor of shape (batch_size, seq_len, embedding_dim),\n",
        "                       representing contextualized embeddings for each token.\n",
        "        \"\"\"\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # Token embedding\n",
        "        x = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        # Scale embeddings\n",
        "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
        "\n",
        "        # Add positional encoding\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        # Pass through stacked EncoderLayers\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training=training, mask=mask)\n",
        "\n",
        "        return x  # (batch_size, seq_len, embedding_dim)\n"
      ],
      "metadata": {
        "id": "OJQVcCU9qYpk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestEncoder(unittest.TestCase):\n",
        "    def test_encoder_output_shape(self):\n",
        "        batch_size = 2\n",
        "        seq_len = 5\n",
        "        vocab_size = 50\n",
        "        embedding_dim = 16\n",
        "        num_layers = 2\n",
        "        num_heads = 4\n",
        "        ffn_dim = 32\n",
        "        max_pos_encoding = 10\n",
        "\n",
        "        encoder = Encoder(\n",
        "            num_layers=num_layers,\n",
        "            embedding_dim=embedding_dim,\n",
        "            num_heads=num_heads,\n",
        "            fully_connected_dim=ffn_dim,\n",
        "            input_vocab_size=vocab_size,\n",
        "            maximum_position_encoding=max_pos_encoding\n",
        "        )\n",
        "\n",
        "        # Random token IDs\n",
        "        x_input = tf.constant(np.random.randint(0, vocab_size, (batch_size, seq_len)), dtype=tf.int32)\n",
        "\n",
        "        # Forward pass\n",
        "        output = encoder(x_input, training=False, mask=None)\n",
        "\n",
        "        # Check output shape\n",
        "        self.assertEqual(output.shape, (batch_size, seq_len, embedding_dim))\n",
        "\n",
        "unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(TestEncoder))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ooiw97CqhqX",
        "outputId": "a85a7d31-9071-4528-cbdd-2f2761727fc3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".\n",
            "----------------------------------------------------------------------\n",
            "Ran 1 test in 0.729s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Transformer Decoder Layer consisting of:\n",
        "    1. Masked self-attention (with look-ahead mask)\n",
        "    2. Encoder-Decoder attention (using encoder output)\n",
        "    3. Feed-forward network\n",
        "    Each sub-layer uses residual connections followed by layer normalization.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 embedding_dim: int,\n",
        "                 num_heads: int,\n",
        "                 fully_connected_dim: int,\n",
        "                 dropout_rate: float = 0.1,\n",
        "                 layernorm_eps: float = 1e-6):\n",
        "        \"\"\"\n",
        "        Initializes the DecoderLayer.\n",
        "\n",
        "        Arguments:\n",
        "            embedding_dim (int): Dimensionality of the embeddings.\n",
        "            num_heads (int): Number of attention heads.\n",
        "            fully_connected_dim (int): Hidden size of the feed-forward network.\n",
        "            dropout_rate (float, optional): Dropout rate applied to FFN. Defaults to 0.1.\n",
        "            layernorm_eps (float, optional): Epsilon value for LayerNormalization. Defaults to 1e-6.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(num_heads=num_heads,\n",
        "                                       key_dim=embedding_dim,\n",
        "                                       dropout=dropout_rate)\n",
        "        self.mha2 = MultiHeadAttention(num_heads=num_heads,\n",
        "                                       key_dim=embedding_dim,\n",
        "                                       dropout=dropout_rate)\n",
        "\n",
        "        self.ffn = FullyConnected(embedding_dim=embedding_dim,\n",
        "                                  fully_connected_dim=fully_connected_dim)\n",
        "\n",
        "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
        "        self.layernorm3 = LayerNormalization(epsilon=layernorm_eps)\n",
        "\n",
        "        self.dropout_ffn = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self,\n",
        "             x: tf.Tensor,\n",
        "             enc_output: tf.Tensor,\n",
        "             training: bool,\n",
        "             look_ahead_mask: Optional[tf.Tensor] = None,\n",
        "             padding_mask: Optional[tf.Tensor] = None\n",
        "            ) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass for the Decoder Layer.\n",
        "\n",
        "        Arguments:\n",
        "            x (tf.Tensor): Input tensor of shape (batch_size, target_seq_len, embedding_dim).\n",
        "            enc_output (tf.Tensor): Encoder output of shape (batch_size, input_seq_len, embedding_dim).\n",
        "            training (bool): Flag to enable dropout during training.\n",
        "            look_ahead_mask (tf.Tensor, optional): Mask to prevent attention to future positions.\n",
        "            padding_mask (tf.Tensor, optional): Mask to ignore padding in encoder-decoder attention.\n",
        "\n",
        "        Returns:\n",
        "            Tuple containing:\n",
        "                out3 (tf.Tensor): Output tensor (batch_size, target_seq_len, embedding_dim).\n",
        "                attn_weights_block1 (tf.Tensor): Attention weights from masked self-attention.\n",
        "                attn_weights_block2 (tf.Tensor): Attention weights from encoder-decoder attention.\n",
        "        \"\"\"\n",
        "        # BLOCK 1: Masked Self-Attention\n",
        "        attn_output1, attn_weights_block1 = self.mha1(\n",
        "            query=x, value=x, key=x, attention_mask=look_ahead_mask, return_attention_scores=True\n",
        "        )\n",
        "        out1 = self.layernorm1(x + attn_output1)\n",
        "\n",
        "        # BLOCK 2: Encoder-Decoder Attention\n",
        "        attn_output2, attn_weights_block2 = self.mha2(\n",
        "            query=out1, value=enc_output, key=enc_output, attention_mask=padding_mask, return_attention_scores=True\n",
        "        )\n",
        "        out2 = self.layernorm2(out1 + attn_output2)\n",
        "\n",
        "        # BLOCK 3: Feed-Forward Network\n",
        "        ffn_output = self.ffn(out2)\n",
        "        ffn_output = self.dropout_ffn(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(out2 + ffn_output)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2\n"
      ],
      "metadata": {
        "id": "f-t9G4ewPust"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDecoderLayer(unittest.TestCase):\n",
        "    def test_decoder_layer_shapes(self):\n",
        "        batch_size = 2\n",
        "        target_seq_len = 5\n",
        "        input_seq_len = 6\n",
        "        embedding_dim = 8\n",
        "        num_heads = 2\n",
        "        fully_connected_dim = 16\n",
        "\n",
        "        x = tf.random.uniform((batch_size, target_seq_len, embedding_dim))\n",
        "        enc_output = tf.random.uniform((batch_size, input_seq_len, embedding_dim))\n",
        "\n",
        "        layer = DecoderLayer(embedding_dim, num_heads, fully_connected_dim)\n",
        "        out, attn1, attn2 = layer(x, enc_output, training=False)\n",
        "\n",
        "        # Check output shape\n",
        "        self.assertEqual(out.shape, (batch_size, target_seq_len, embedding_dim))\n",
        "\n",
        "        # Check attention weights shapes\n",
        "        self.assertEqual(attn1.shape, (batch_size, num_heads, target_seq_len, target_seq_len))\n",
        "        self.assertEqual(attn2.shape, (batch_size, num_heads, target_seq_len, input_seq_len))\n",
        "\n",
        "# Run the tests directly in Colab\n",
        "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsASybwQQtE4",
        "outputId": "1b12665c-f225-492b-ab69-404a85148d6d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "..............\n",
            "----------------------------------------------------------------------\n",
            "Ran 14 tests in 1.024s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x781f6e0d28a0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Transformer Decoder consisting of:\n",
        "    1. Token embedding + positional encoding\n",
        "    2. Stack of DecoderLayers\n",
        "    3. Dropout applied to embeddings\n",
        "    4. Collects attention weights from each decoder layer\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_layers: int,\n",
        "                 embedding_dim: int,\n",
        "                 num_heads: int,\n",
        "                 fully_connected_dim: int,\n",
        "                 target_vocab_size: int,\n",
        "                 maximum_position_encoding: int,\n",
        "                 dropout_rate: float = 0.1,\n",
        "                 layernorm_eps: float = 1e-6):\n",
        "        \"\"\"\n",
        "        Initializes the Decoder.\n",
        "\n",
        "        Arguments:\n",
        "            num_layers (int): Number of decoder layers to stack.\n",
        "            embedding_dim (int): Dimensionality of embeddings.\n",
        "            num_heads (int): Number of attention heads.\n",
        "            fully_connected_dim (int): Hidden size of feed-forward network.\n",
        "            target_vocab_size (int): Vocabulary size of target language.\n",
        "            maximum_position_encoding (int): Maximum number of positions for positional encoding.\n",
        "            dropout_rate (float, optional): Dropout rate. Defaults to 0.1.\n",
        "            layernorm_eps (float, optional): Epsilon for layer normalization. Defaults to 1e-6.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = Embedding(target_vocab_size, self.embedding_dim)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
        "\n",
        "        self.dec_layers = [\n",
        "            DecoderLayer(\n",
        "                embedding_dim=self.embedding_dim,\n",
        "                num_heads=num_heads,\n",
        "                fully_connected_dim=fully_connected_dim,\n",
        "                dropout_rate=dropout_rate,\n",
        "                layernorm_eps=layernorm_eps\n",
        "            )\n",
        "            for _ in range(self.num_layers)\n",
        "        ]\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self,\n",
        "         x: tf.Tensor,\n",
        "         enc_output: tf.Tensor,\n",
        "         training: bool,\n",
        "         look_ahead_mask: Optional[tf.Tensor] = None,\n",
        "         padding_mask: Optional[tf.Tensor] = None\n",
        "        ) -> Tuple[tf.Tensor, Dict[str, tf.Tensor]]:\n",
        "      \"\"\"\n",
        "      Forward pass for the Transformer Decoder.\n",
        "\n",
        "      This method performs the following steps:\n",
        "      1. Embeds the input tokens and scales the embeddings.\n",
        "      2. Adds positional encodings to the embeddings.\n",
        "      3. Applies dropout to the embeddings.\n",
        "      4. Passes the embeddings through a stack of DecoderLayers.\n",
        "      5. Collects attention weights from each decoder layer for analysis.\n",
        "\n",
        "      Arguments:\n",
        "          x (tf.Tensor): Input target tensor of shape (batch_size, target_seq_len),\n",
        "                        containing token IDs.\n",
        "          enc_output (tf.Tensor): Output from the encoder of shape\n",
        "                                  (batch_size, input_seq_len, embedding_dim).\n",
        "          training (bool): Flag indicating whether in training mode (affects dropout).\n",
        "          look_ahead_mask (Optional[tf.Tensor]): Mask to prevent attention to future positions.\n",
        "          padding_mask (Optional[tf.Tensor]): Mask to ignore padding positions in encoder-decoder attention.\n",
        "\n",
        "      Returns:\n",
        "          Tuple containing:\n",
        "              x (tf.Tensor): Output of the decoder, shape (batch_size, target_seq_len, embedding_dim).\n",
        "              attention_weights (Dict[str, tf.Tensor]): Dictionary containing attention weights\n",
        "                  from each decoder layer:\n",
        "                  - 'decoder_layer{i}_block1_self_att': Self-attention weights.\n",
        "                  - 'decoder_layer{i}_block2_decenc_att': Encoder-decoder attention weights.\n",
        "      \"\"\"\n",
        "      seq_len = tf.shape(x)[1]\n",
        "      attention_weights = {}\n",
        "\n",
        "      # Token embedding + scale\n",
        "      x = self.embedding(x)\n",
        "      x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
        "\n",
        "      # Add positional encoding\n",
        "      x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "      # Apply dropout\n",
        "      x = self.dropout(x, training=training)\n",
        "\n",
        "      # Pass through decoder layers\n",
        "      for i, dec_layer in enumerate(self.dec_layers):\n",
        "          x, block1, block2 = dec_layer(\n",
        "              x,\n",
        "              enc_output,\n",
        "              training=training,\n",
        "              look_ahead_mask=look_ahead_mask,\n",
        "              padding_mask=padding_mask\n",
        "          )\n",
        "          attention_weights[f'decoder_layer{i+1}_block1_self_att'] = block1\n",
        "          attention_weights[f'decoder_layer{i+1}_block2_decenc_att'] = block2\n",
        "\n",
        "      return x, attention_weights\n"
      ],
      "metadata": {
        "id": "SLO81Fm5RRE2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDecoder(unittest.TestCase):\n",
        "    def test_decoder_shapes(self):\n",
        "        batch_size = 2\n",
        "        target_seq_len = 5\n",
        "        input_seq_len = 6\n",
        "        embedding_dim = 8\n",
        "        num_heads = 2\n",
        "        fully_connected_dim = 16\n",
        "        num_layers = 2\n",
        "        vocab_size = 50\n",
        "        max_pos_encoding = 10\n",
        "\n",
        "        x = tf.random.uniform((batch_size, target_seq_len), maxval=vocab_size, dtype=tf.int32)\n",
        "        enc_output = tf.random.uniform((batch_size, input_seq_len, embedding_dim))\n",
        "\n",
        "        decoder = Decoder(num_layers, embedding_dim, num_heads, fully_connected_dim,\n",
        "                          vocab_size, max_pos_encoding)\n",
        "        dec_output, attn_weights = decoder(x, enc_output, training=False)\n",
        "\n",
        "        self.assertEqual(dec_output.shape, (batch_size, target_seq_len, embedding_dim))\n",
        "        self.assertEqual(len(attn_weights), num_layers * 2)\n",
        "        for key, value in attn_weights.items():\n",
        "            self.assertEqual(value.shape[0], batch_size)  # batch_size matches\n",
        "            self.assertEqual(value.shape[2], target_seq_len)  # seq_len_q matches\n",
        "\n",
        "# Run the test in Colab\n",
        "unittest.main(argv=['first-arg-is-ignored'], exit=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg9fL-lYRnwa",
        "outputId": "eb51586d-2da2-4df0-dca1-2822893093af"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "...............\n",
            "----------------------------------------------------------------------\n",
            "Ran 15 tests in 1.772s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x781f6e0d2840>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Complete Transformer model combining an Encoder and a Decoder.\n",
        "    This model can be used for sequence-to-sequence tasks such as machine translation.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_layers: int,\n",
        "                 embedding_dim: int,\n",
        "                 num_heads: int,\n",
        "                 fully_connected_dim: int,\n",
        "                 input_vocab_size: int,\n",
        "                 target_vocab_size: int,\n",
        "                 max_positional_encoding_input: int,\n",
        "                 max_positional_encoding_target: int,\n",
        "                 dropout_rate: float = 0.1,\n",
        "                 layernorm_eps: float = 1e-6):\n",
        "        \"\"\"\n",
        "        Initializes the Transformer model with encoder, decoder, and final linear layer.\n",
        "\n",
        "        Args:\n",
        "            num_layers (int): Number of encoder and decoder layers.\n",
        "            embedding_dim (int): Dimensionality of embeddings.\n",
        "            num_heads (int): Number of attention heads.\n",
        "            fully_connected_dim (int): Hidden size of feed-forward networks.\n",
        "            input_vocab_size (int): Vocabulary size for input.\n",
        "            target_vocab_size (int): Vocabulary size for output.\n",
        "            max_positional_encoding_input (int): Maximum positional encoding for input.\n",
        "            max_positional_encoding_target (int): Maximum positional encoding for target.\n",
        "            dropout_rate (float, optional): Dropout rate. Defaults to 0.1.\n",
        "            layernorm_eps (float, optional): Epsilon for LayerNormalization. Defaults to 1e-6.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            num_layers=num_layers,\n",
        "            embedding_dim=embedding_dim,\n",
        "            num_heads=num_heads,\n",
        "            fully_connected_dim=fully_connected_dim,\n",
        "            input_vocab_size=input_vocab_size,\n",
        "            maximum_position_encoding=max_positional_encoding_input,\n",
        "            dropout_rate=dropout_rate,\n",
        "            layernorm_eps=layernorm_eps\n",
        "        )\n",
        "\n",
        "        self.decoder = Decoder(\n",
        "            num_layers=num_layers,\n",
        "            embedding_dim=embedding_dim,\n",
        "            num_heads=num_heads,\n",
        "            fully_connected_dim=fully_connected_dim,\n",
        "            target_vocab_size=target_vocab_size,\n",
        "            maximum_position_encoding=max_positional_encoding_target,\n",
        "            dropout_rate=dropout_rate,\n",
        "            layernorm_eps=layernorm_eps\n",
        "        )\n",
        "\n",
        "        self.final_layer = Dense(target_vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self,\n",
        "             input_sentence: tf.Tensor,\n",
        "             output_sentence: tf.Tensor,\n",
        "             training: bool,\n",
        "             enc_padding_mask: Optional[tf.Tensor] = None,\n",
        "             look_ahead_mask: Optional[tf.Tensor] = None,\n",
        "             dec_padding_mask: Optional[tf.Tensor] = None\n",
        "            ) -> Tuple[tf.Tensor, Dict[str, tf.Tensor]]:\n",
        "        \"\"\"\n",
        "        Forward pass of the Transformer.\n",
        "\n",
        "        Args:\n",
        "            input_sentence (tf.Tensor): Input tensor of shape (batch_size, input_seq_len) with token indices.\n",
        "            output_sentence (tf.Tensor): Target tensor of shape (batch_size, target_seq_len) with token indices.\n",
        "            training (bool): Whether the model is in training mode (affects dropout).\n",
        "            enc_padding_mask (Optional[tf.Tensor]): Mask for encoder input padding tokens.\n",
        "            look_ahead_mask (Optional[tf.Tensor]): Mask to prevent attention to future positions.\n",
        "            dec_padding_mask (Optional[tf.Tensor]): Mask for decoder attention to encoder output padding.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[tf.Tensor, Dict[str, tf.Tensor]]:\n",
        "                - final_output: Tensor of shape (batch_size, target_seq_len, target_vocab_size)\n",
        "                - attention_weights: Dictionary of attention weights from each decoder layer.\n",
        "        \"\"\"\n",
        "        # Encoder\n",
        "        enc_output = self.encoder(\n",
        "            x=input_sentence,\n",
        "            training=training,\n",
        "            mask=enc_padding_mask\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        dec_output, attention_weights = self.decoder(\n",
        "            x=output_sentence,\n",
        "            enc_output=enc_output,\n",
        "            training=training,\n",
        "            look_ahead_mask=look_ahead_mask,\n",
        "            padding_mask=dec_padding_mask\n",
        "        )\n",
        "\n",
        "        # Final linear + softmax\n",
        "        final_output = self.final_layer(dec_output)\n",
        "\n",
        "        return final_output, attention_weights\n"
      ],
      "metadata": {
        "id": "ej3dEYuHTIA6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestTransformer(unittest.TestCase):\n",
        "    def test_transformer_shapes(self):\n",
        "        batch_size = 2\n",
        "        input_seq_len = 6\n",
        "        target_seq_len = 5\n",
        "        embedding_dim = 8\n",
        "        num_heads = 2\n",
        "        fully_connected_dim = 16\n",
        "        num_layers = 2\n",
        "        input_vocab_size = 50\n",
        "        target_vocab_size = 60\n",
        "        max_pos_encoding_input = 10\n",
        "        max_pos_encoding_target = 10\n",
        "\n",
        "        input_sentence = tf.random.uniform((batch_size, input_seq_len), maxval=input_vocab_size, dtype=tf.int32)\n",
        "        output_sentence = tf.random.uniform((batch_size, target_seq_len), maxval=target_vocab_size, dtype=tf.int32)\n",
        "\n",
        "        transformer = Transformer(num_layers, embedding_dim, num_heads, fully_connected_dim,\n",
        "                                  input_vocab_size, target_vocab_size,\n",
        "                                  max_pos_encoding_input, max_pos_encoding_target)\n",
        "\n",
        "        final_output, attn_weights = transformer(input_sentence, output_sentence, training=False)\n",
        "\n",
        "        self.assertEqual(final_output.shape, (batch_size, target_seq_len, target_vocab_size))\n",
        "        self.assertEqual(len(attn_weights), num_layers * 2)  # 2 attention blocks per decoder layer\n",
        "        for key, value in attn_weights.items():\n",
        "            self.assertEqual(value.shape[0], batch_size)\n",
        "\n",
        "# Run the test\n",
        "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hr4Eye8WTQcd",
        "outputId": "34227b09-0b1b-410f-bf9a-a6d4999bc06c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "................\n",
            "----------------------------------------------------------------------\n",
            "Ran 16 tests in 4.884s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x781f6e255790>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}